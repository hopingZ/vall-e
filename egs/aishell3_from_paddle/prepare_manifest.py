import argparse
import jsonlines
from tqdm import tqdm
from pathlib import Path
from functools import partial
from lhotse import Recording, RecordingSet, SupervisionSet, SupervisionSegment, validate_recordings_and_supervisions


def main():
    # parse config and args
    parser = argparse.ArgumentParser(
        description="Make manifest from dump dir generated by PaddleSpeech."
    )
    parser.add_argument(
        "--dl_dir", default=None, type=str, help="directory to dataset."
    )
    parser.add_argument(
        "--dump_dir", default=None, type=str, help="directory to dump dir."
    )
    parser.add_argument(
        "--output_dir", default=None, type=str, help="directory to output dir."
    )
    args = parser.parse_args()
    dl_dir = Path(args.dl_dir)
    dump_dir = Path(args.dump_dir)
    output_dir = Path(args.output_dir)

    for part in ["train", "dev", "test"]:
        raw_metadata_file_path = dump_dir / part / "raw/metadata.jsonl"
        norm_metadata_file_path = dump_dir / part / "norm/metadata.jsonl"

        with jsonlines.open(raw_metadata_file_path, 'r') as reader:
            raw_metadata = list(reader)
        with jsonlines.open(norm_metadata_file_path, 'r') as reader:
            norm_metadata = list(reader)

        wav_file_paths_of_cur_part = []
        supervisions_of_cur_part = []

        for raw_metadata_i in raw_metadata:
            wav_file_paths_of_cur_part.append(
                dl_dir / "train/wav" / raw_metadata_i["speaker"] / (raw_metadata_i["utt_id"] + ".wav")
            )

        file_read_worker = partial(
            Recording.from_file,
            force_opus_sampling_rate=None,
            recording_id=None,
        )
        recordings = RecordingSet.from_recordings(
            tqdm(map(file_read_worker, wav_file_paths_of_cur_part),
                 desc="Collecting audio files for %s part" % part)
        )

        for i in range(len(raw_metadata)):
            raw_metadata_i = raw_metadata[i]
            norm_metadata_i = norm_metadata[i]

            utt_id = raw_metadata_i["utt_id"]
            phones = raw_metadata_i["phones"]
            speaker = raw_metadata_i["speaker"]
            phone_tokens = norm_metadata_i["text"]

            customd = {
                "phones": phones,
                "phone_tokens": phone_tokens
            }
            supervisions_of_cur_part.append(
                SupervisionSegment(
                    id=utt_id,
                    recording_id=utt_id,
                    start=0.0,
                    duration=recordings[utt_id].duration,
                    channel=0,
                    text=phones,
                    language="Mandarin",
                    speaker=speaker,
                    gender=None,
                    custom=customd,
                )
            )

        supervisions = SupervisionSet.from_segments(supervisions_of_cur_part)
        validate_recordings_and_supervisions(recordings, supervisions)

        if output_dir is not None:
            supervisions.to_file(output_dir / f"aishell3_supervisions_{part}.jsonl.gz")
            recordings.to_file(output_dir / f"aishell3_recordings_{part}.jsonl.gz")


if __name__ == "__main__":
    main()

